# data_eng_instructions
Test task for J&amp;J


java -version - be sure spark is working above jvm (scala native)
pip install pyspark


python main.py

For testing:
pip install nose
pip install pytest
-> restart ide -> invalidate caches

Run pytest in bash

For pyspark: - install java 17
sdk install java 17.0.11.fx-zulu -> for example
pip install "pyarrow>=15.0.0"
pip install "grpcio>=1.48.1"
pip install "grpcio-status >= 1.48.1"
pip install "zstandard >= 0.25.0"

TODO:
simplifications: operator not related, many-to-many not related

prof of concept project -> sdt1, sdt2 , sdt3. Partitioning, indexing etc..
assumptions. Mini-dimentios, skill-level (not extracted)

write requirements explanations
add dataframes cleaning

was stubed with -1 - but dementions